{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0f1699e3-84ac-4f24-a97f-21ea3b3768ef",
   "metadata": {},
   "source": [
    "# Unit 1 Assignment: The Model Benchmark Challenge\n",
    "\n",
    "**Objective**: In this assignment, you will step beyond simply using a model and instead evaluate the architectural differences between **BERT**, **RoBERTa**, and **BART**. You will force these models to perform tasks they might not be designed for, to observe why architecture matters.\n",
    "\n",
    "**Instructions**:\n",
    "1.  Create a new Jupyter Notebook (e.g., `Unit1_Benchmark.ipynb`).\n",
    "2.  Install/Import `transformers` and `pipeline`.\n",
    "3.  Complete the 3 Experiments listed below using **all three models** for each task.\n",
    "4.  Fill out the **Observation Table** with your results.\n",
    "\n",
    "---\n",
    "\n",
    "### The Models to Test\n",
    "1.  **BERT** (`bert-base-uncased`): An **Encoder-only** model (designed for understanding, not generation).\n",
    "2.  **RoBERTa** (`roberta-base`): An optimized **Encoder-only** model.\n",
    "3.  **BART** (`facebook/bart-base`): An **Encoder-Decoder** model (designed for seq2seq tasks like translation/generation).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854308e6-a387-4d47-94f7-bb778c129057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Experiment 1: Text Generation\n",
    "#Task: Try to generate text using the prompt: `\"The future of Artificial Intelligence is\"`\n",
    "#Code Hint: `pipeline('text-generation', model='...')`\n",
    "#Hypothesis: Which models will fail? Why? (Hint: Can an Encoder *generate* new tokens easily?)\n",
    "from transformers import pipeline\n",
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "gen_bert = pipeline(\"text-generation\", model=\"bert-base-uncased\")\n",
    "gen_bert(prompt, max_length=50)\n",
    "#here bert model is failed because BERT is an encoder model, it can only understand text and it can't generate new sentences.\n",
    "#encoder model is made to understand text , not to generate new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ad99de-2310-481e-9c45-4c4c8b65bcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RoBERTa Output:\n",
      "[{'generated_text': 'The future of Artificial Intelligence is'}]\n"
     ]
    }
   ],
   "source": [
    "### Experiment 1: Text Generation\n",
    "#Task: Try to generate text using the prompt: `\"The future of Artificial Intelligence is\"`\n",
    "#Code Hint: `pipeline('text-generation', model='...')`\n",
    "#Hypothesis: Which models will fail? Why? (Hint: Can an Encoder *generate* new tokens easily?)\n",
    "from transformers import pipeline\n",
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "gen_roberta = pipeline(\"text-generation\", model=\"roberta-base\")\n",
    "print(\"\\nRoBERTa Output:\")\n",
    "print(gen_roberta(prompt, max_length=50))\n",
    "#robert is also a encoder model , which means it is designed to understand text, not to generate or continue sentences.\n",
    "#so it cannot produce new words after the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7d2f77-70f7-4154-a3a6-531844f26a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BART Output:\n",
      "[{'generated_text': 'The future of Artificial Intelligence is BITOverrideOverridetgtg Michigan NVAST BIT BIT BIT242110242henko242 NV NV NVエ NVる NVpol Virus242 NV242242 NV Dunham NV off242 Mart Martpolprem� combo NV NV Classification NV242 NV� NV NV� NV NVpremthrow NVprem242�prempremprem Collins Eternity realistic�prem Collins� NVAST predetermined NV242� NVisersMAG NV NV precarious NV off���premoilerpremprem precarious� off�premcfcf�premエ swornMAG exceptionallyprem exceptionally cannabinoidsprem� premium premium�MAGprem�cfcfprem�� assassins premium�� Briancfcf NV� timelinecfpremcf��cf cannabinoidspremprem premium premiumcfcf kilograms242 Brian� exceptionallypremprem�usted Briancfprocesscfcf timeline� NVNap precarious��エ�� premium off� Brian kilogramsRollprem precariousprem swornNapcf Brian��oilercfcf offnell timeline kilograms�cfprem sworn� precarious� precariouscf NVcf� SUR SURnell premium� cannabinoidscf�cfjing Brian�bigbignellcfnell�cf premiumNapcf cannabinoidscfoiler precarious�cf242nellbig timelinebigbig cannabinoidsbigcf cannabinoids cannabinoids premiumbigprocess timeline� timeline�cfnell timeline� cannabinoidsbigbig Briannellnell'}]\n"
     ]
    }
   ],
   "source": [
    "### Experiment 1: Text Generation\n",
    "#Task: Try to generate text using the prompt: `\"The future of Artificial Intelligence is\"`\n",
    "#Code Hint: `pipeline('text-generation', model='...')`\n",
    "#Hypothesis: Which models will fail? Why? (Hint: Can an Encoder *generate* new tokens easily?)\n",
    "from transformers import pipeline\n",
    "prompt = \"The future of Artificial Intelligence is\"\n",
    "gen_bart = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
    "print(\"\\nBART Output:\")\n",
    "print(gen_bart(prompt, max_length=50))\n",
    "#bart uses both encoder and decoder,but it's not trained mainly for free text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fbd7023-6830-44a3-8fbf-da577f283f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Output:\n",
      "[{'score': 0.539692759513855, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575766563415527, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}, {'score': 0.05405496060848236, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to produce new content.'}, {'score': 0.044515229761600494, 'token': 4503, 'token_str': 'develop', 'sequence': 'the goal of generative ai is to develop new content.'}, {'score': 0.017577484250068665, 'token': 5587, 'token_str': 'add', 'sequence': 'the goal of generative ai is to add new content.'}]\n"
     ]
    }
   ],
   "source": [
    "### Experiment 2: Masked Language Modeling (Missing Word)\n",
    "#Task**: Predict the missing word in: `\"The goal of Generative AI is to [MASK] new content.\"`\n",
    "#   **Code Hint**: `pipeline('fill-mask', model='...')`\n",
    "#   **Hypothesis**: This is how BERT/RoBERTa were trained. They should perform well.\n",
    "from transformers import pipeline\n",
    "# BERT\n",
    "fill_bert = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "print(\"BERT Output:\")\n",
    "print(fill_bert(\"The goal of Generative AI is to [MASK] new content.\"))\n",
    "#BERT and RoBERTa are best for predicting missing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "382a1df8-8e0b-4bd9-96f7-78ca9b9bdaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RoBERTa Output:\n",
      "[{'score': 0.37113118171691895, 'token': 5368, 'token_str': ' generate', 'sequence': 'The goal of Generative AI is to generate new content.'}, {'score': 0.3677138090133667, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.08351466804742813, 'token': 8286, 'token_str': ' discover', 'sequence': 'The goal of Generative AI is to discover new content.'}, {'score': 0.02133519947528839, 'token': 465, 'token_str': ' find', 'sequence': 'The goal of Generative AI is to find new content.'}, {'score': 0.01652175933122635, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}]\n"
     ]
    }
   ],
   "source": [
    "### Experiment 2: Masked Language Modeling (Missing Word)\n",
    "#Task**: Predict the missing word in: `\"The goal of Generative AI is to [MASK] new content.\"`\n",
    "#   **Code Hint**: `pipeline('fill-mask', model='...')`\n",
    "#   **Hypothesis**: This is how BERT/RoBERTa were trained. They should perform well.\n",
    "from transformers import pipeline\n",
    "# RoBERTa\n",
    "fill_roberta = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
    "print(\"\\nRoBERTa Output:\")\n",
    "print(fill_roberta(\"The goal of Generative AI is to <mask> new content.\"))\n",
    "#BERT and RoBERTa are best for predicting missing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a5dbec7-452c-4145-96b8-347db756523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BART Output:\n",
      "[{'score': 0.07461544126272202, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.06571853160858154, 'token': 244, 'token_str': ' help', 'sequence': 'The goal of Generative AI is to help new content.'}, {'score': 0.060880184173583984, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}, {'score': 0.035935722291469574, 'token': 3155, 'token_str': ' enable', 'sequence': 'The goal of Generative AI is to enable new content.'}, {'score': 0.03319481760263443, 'token': 1477, 'token_str': ' improve', 'sequence': 'The goal of Generative AI is to improve new content.'}]\n"
     ]
    }
   ],
   "source": [
    "### Experiment 2: Masked Language Modeling (Missing Word)\n",
    "#Task**: Predict the missing word in: `\"The goal of Generative AI is to [MASK] new content.\"`\n",
    "#   **Code Hint**: `pipeline('fill-mask', model='...')`\n",
    "#   **Hypothesis**: This is how BERT/RoBERTa were trained. They should perform well.\n",
    "fill_bart = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
    "print(\"\\nBART Output:\")\n",
    "print(fill_bart(\"The goal of Generative AI is to <mask> new content.\"))\n",
    "#BART can do it but not as strong as BERT and RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d084b95-8ab0-4612-b819-b7c64f77da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Answer: hallucinations, bias, and deepfakes\n"
     ]
    }
   ],
   "source": [
    "### Experiment 3: Question Answering\n",
    "#Task**: Answer the question `\"What are the risks?\"` based on the context: `\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"`\n",
    "#Code Hint**: `pipeline('question-answering', model='...')`\n",
    "#Note**: Using a \"base\" model (not fine-tuned for SQuAD) might yield random or poor results. Observe this behavior.\n",
    "from transformers import pipeline\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "bert_qa = pipeline('question-answering', model='bert-base-uncased', tokenizer='bert-base-uncased')\n",
    "result_bert = bert_qa(question=question, context=context)\n",
    "print(\"BERT Answer:\", result_bert['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae4d8bb-5ae3-4541-82f0-409b0ff5d044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa Answer: deepfakes\n"
     ]
    }
   ],
   "source": [
    "### Experiment 3: Question Answering\n",
    "#Task**: Answer the question `\"What are the risks?\"` based on the context: `\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"`\n",
    "#Code Hint**: `pipeline('question-answering', model='...')`\n",
    "#Note**: Using a \"base\" model (not fine-tuned for SQuAD) might yield random or poor results. Observe this behavior.\n",
    "from transformers import pipeline\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "roberta_qa = pipeline('question-answering', model='roberta-base', tokenizer='roberta-base')\n",
    "result_roberta = roberta_qa(question=question, context=context)\n",
    "print(\"RoBERTa Answer:\", result_roberta['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa707e78-3651-45ef-9fdc-1567860bbdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Answer: poses significant\n"
     ]
    }
   ],
   "source": [
    "### Experiment 3: Question Answering\n",
    "#Task**: Answer the question `\"What are the risks?\"` based on the context: `\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"`\n",
    "#Code Hint**: `pipeline('question-answering', model='...')`\n",
    "#Note**: Using a \"base\" model (not fine-tuned for SQuAD) might yield random or poor results. Observe this behavior.\n",
    "from transformers import pipeline\n",
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n",
    "bart_qa = pipeline('question-answering', model='facebook/bart-base', tokenizer='facebook/bart-base')\n",
    "result_bart = bart_qa(question=question, context=context)\n",
    "print(\"BART Answer:\", result_bart['answer'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04b24aee-6044-4074-9317-49bf467746ca",
   "metadata": {},
   "source": [
    "| Task | Model | Classification(Success/Failure)|Observation (What actually happened?)| Why did this happen? (Architectural Reason)|\n",
    "| TG   | BERT  | Failure                        | Generated dots or meaningless output| BERT is an encoder-only model and is not   \n",
    "|      |       |                                | instead of a proper sentence.       |  designed to generate text.\n",
    "| TG   |ROBERT | Failure                        | Returned only the prompt or stopped | RoBERTa is also an encoder-only model,  \n",
    "|      |       |                                | without adding new words.           |  so it cannot generate new tokens.\n",
    "| TG   | BART  | Partial Success                | Generated text, but it was mostly   | BART is an encoder–decoder model,but it is \n",
    "|      |       |                                |       random and meaningless.       | trained for tasks like summarization,not \n",
    "|      |       |                                |                                     | free text generation.\n",
    "| MW   | BERT  | Success                        |Predicted`create`confidently,other   |BERT is trained for masked language modeling\n",
    "|      |       |                                |goodoptions too(`generate`,`produce`)|,so it’s made to guess missing words. \n",
    "| MW   |RoBERTa| Success                        | Predicted`generate`and `create`     |RoBERTa is like BERT but trained on more data\n",
    "|      |       |                                |almost equally confident             |and slightly different setup,still masked LM |      |       |                                |                                     |                          friendly\n",
    "| MW   | BART  | Partial Success                | Predicted `create`,`help`,`provide`,| BART is a **seq2seq model**, not mainly    |      |       |                                | scores are low                      |formasked words, so confidence is lower.     |QA    |BERT   | Success                        |Got full correct answer:             |BERT understands context pretty well even\n",
    "|      |       |                                |hallucinations,bias, and deepfakes   |without extra training\n",
    "|QA    |RoBERTa| Partial                        |Only got deepfakes                   |Not fine-tuned for QA,so it misses part of\n",
    "|      |       |                                |                                     |the answer\n",
    "|QA    |BART   | Failure                        |Got wrong answer:poses significant   |BART base is not meant for QA without \n",
    "|      |       |                                |                                     | Fine-tuning\n",
    "|      |       |                                |                                     |\n",
    "|      |       |                                |                                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7fc1e6-5f81-4aef-b882-e4896f5c4a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
